{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AREP-L5: Embeddings and Text Processing Pipeline\n",
    "This notebook implements the foundational data pipeline for LLMs, moving from raw text to continuous vectors via BPE, sliding windows, and embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Text Ingestion\n",
    "Load the text corpus (`the-verdict.txt`) as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0595ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "First 100 chars: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "file_path = \"./src/the-verdict.txt\"\n",
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {file_path} not found. Ensure it is downloaded.\")\n",
    "\n",
    "print(\"Total characters:\", len(raw_text))\n",
    "print(\"First 100 chars:\", raw_text[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767ebe1",
   "metadata": {},
   "source": [
    "## 2. Advanced Tokenization with BPE\n",
    "Instantiate the Byte Pair Encoding (BPE) tokenizer used in GPT architectures.\n",
    "\n",
    "**Why it matters:** BPE limits vocabulary explosion and eliminates out-of-vocabulary (OOV) errors by decomposing unknown words into common subword tokens. This highly compresses sequence lengths to fit inside the LLM's finite context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad15de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in corpus: 5145\n",
      "Original: Hello, do you like AI?\n",
      "Encoded : [15496, 11, 466, 345, 588, 9552, 30]\n",
      "Decoded : Hello, do you like AI?\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize a sample\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(\"Total tokens in corpus:\", len(encoded_text))\n",
    "\n",
    "# Encode-decode\n",
    "sample_text = \"Hello, do you like AI?\"\n",
    "encoded_sample = tokenizer.encode(sample_text)\n",
    "decoded_sample = tokenizer.decode(encoded_sample)\n",
    "\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Encoded :\", encoded_sample)\n",
    "print(\"Decoded :\", decoded_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e71012",
   "metadata": {},
   "source": [
    "## 3. Autoregressive Dataset Construction\n",
    "Implement a PyTorch `Dataset` to generate contextual blocks and shifted targets.\n",
    "\n",
    "**Why it matters:** LLMs must learn to predict the next sequential token. The sliding window constructs a dataset of sequence-to-target pairs, exposing the autoregressive model to various context depths during the optimization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995160ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onecode/.local/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:283: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Slide a window across the tokens\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# Instantiate DataLoader helper\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader, dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df47324",
   "metadata": {},
   "source": [
    "## 4. Experiment: Overlap and Stride\n",
    "Analyze the impact of modifying `max_length` and `stride`, starting with a baseline (`max_length=4`, `stride=4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc821b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Samples (max_length=4, stride=4): 1286\n",
      "Tokens [Input]:  tensor([  40,  367, 2885, 1464])\n",
      "Tokens [Target]: tensor([ 367, 2885, 1464, 1807])\n"
     ]
    }
   ],
   "source": [
    "dataloader_base, dataset_base = create_dataloader_v1(\n",
    "    raw_text, max_length=4, stride=4, batch_size=2, shuffle=False\n",
    ")\n",
    "print(\"Baseline Samples (max_length=4, stride=4):\", len(dataset_base))\n",
    "sample_input, sample_target = dataset_base[0]\n",
    "print(\"Tokens [Input]: \", sample_input)\n",
    "print(\"Tokens [Target]:\", sample_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b155f",
   "metadata": {},
   "source": [
    "Run the experiment with high overlap (`max_length=4`, `stride=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643e2102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Samples (max_length=4, stride=1): 5141\n",
      "Tokens [Input]:  tensor([  40,  367, 2885, 1464])\n",
      "Tokens [Target]: tensor([ 367, 2885, 1464, 1807])\n"
     ]
    }
   ],
   "source": [
    "dataloader_exp, dataset_exp = create_dataloader_v1(\n",
    "    raw_text, max_length=4, stride=1, batch_size=2, shuffle=False\n",
    ")\n",
    "print(\"Experiment Samples (max_length=4, stride=1):\", len(dataset_exp))\n",
    "sample_input, sample_target = dataset_exp[0]\n",
    "print(\"Tokens [Input]: \", sample_input)\n",
    "print(\"Tokens [Target]:\", sample_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3335066",
   "metadata": {},
   "source": [
    "## 5. Experiment Results\n",
    "**1. Samples generated:** The baseline (`stride=4`) yields 1286 samples, while the high overlap experiment (`stride=1`) yields 5141 samples.\n",
    "\n",
    "**2. Utility of overlap:** A smaller stride generates overlapping chunks that expose the model to consecutive, unbroken intermediate states. This augments the dataset density and ensures attention mechanisms can map semantic continuity right across sequence boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b829d",
   "metadata": {},
   "source": [
    "## 6. Embedding Layers\n",
    "Combine token embeddings and absolute positional embeddings to form the model input tensor.\n",
    "\n",
    "**Why it matters:** Token embeddings project discrete vocab IDs into dense semantic arrays, avoiding the curse of dimensionality present in one-hot vectors. Positional embeddings re-introduce sequence order coordinates into the parallel, permutation-invariant self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e617bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs shape (B, T): torch.Size([8, 4])\n",
      "Final Embedding Tensor Shape entering Self-Attention: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257     # GPT-2 standard vocab size\n",
    "output_dim = 256       # Dimensionality of the latent continuous space\n",
    "context_length = 1024  # Max sequence length the NN can process\n",
    "\n",
    "# 1. Token Embedding space\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# 2. Positional Embedding space\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# Simulate passing a batch\n",
    "max_len_demo = 4\n",
    "dataloader_demo, _ = create_dataloader_v1(raw_text, batch_size=8, max_length=max_len_demo, stride=4)\n",
    "data_iter = iter(dataloader_demo)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Batch inputs shape (B, T):\", inputs.shape)\n",
    "\n",
    "# Transform inputs\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "# Create position IDs\n",
    "pos_ids = torch.arange(max_len_demo)\n",
    "pos_embeddings = pos_embedding_layer(pos_ids)\n",
    "\n",
    "# Generate input embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"Final Embedding Tensor Shape entering Self-Attention:\", input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9c3c8",
   "metadata": {},
   "source": [
    "## 7. Reflection\n",
    "**Why do embeddings encode meaning, and how are they related to NN concepts?**\n",
    "\n",
    "Embeddings give meaning to words not through definitions, but through context. They are based on the idea that words used in similar sentences tend to mean the same thing.\n",
    "\n",
    "Mathematically, an embedding is simply a matrix of weights (coordinates) within the first layer of the neural network. Initially, everything is random, but as the model trains by trying to predict the next word in a text, it uses backpropagation (error optimization) to push and move these coordinates. Thus, if 'computador' and 'ordenador' are used in the same context, the model's mathematics pushes their coordinates so that they end up very close in multidimensional space. Therefore, for a neural network, 'meaning' is literally a geometric distance created by the need to reduce its prediction errors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
